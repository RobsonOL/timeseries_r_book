[["index.html", "Séries Temporais com R Capítulo 1 Prefácio 1.1 Roteiro 1.2 Pré-requisitos", " Séries Temporais com R Robson Oliveira Lima 2021-12-15 Capítulo 1 Prefácio Este é um livro online que fornece uma breve introdução aos principais métodos de previsão em séries temporais. R é uma linguagem de programação gratuita e bastante popular para análise estatística. Este livro é uma tentativa de traduzir o livro online Forecasting: Principles and Practice, que é uma das principais referências para o ensino de previsão com R. Ao mesmo tempo, muito dos exemplos serão adaptados para utilizar os pacotes timetk (R-timetk?) e modetime (Dancho 2021). Estes dois pacotes tornam a tarefa de realizar previsão muito conveniente. 1.1 Roteiro bla bla 1.2 Pré-requisitos bla bla References "],["os-fundamentos-de-séries-temporais.html", "Capítulo 2 Os fundamentos de séries temporais 2.1 O que pode ser previsto? 2.2 Principais modelos de previsão 2.3 O passo-a-passo de realizar uma previsão 2.4 Um Projeto de Previsão de Ponta a Ponta", " Capítulo 2 Os fundamentos de séries temporais Para que uma criança aprenda a identificar um gato, é preciso que os pais apontem para vários gatos e digam gato. Essa tarefa pode ser repetida algumas poucas vezes por crianças, mas para um modelo de previsão, essa tarefa não é tão simples, e exige uma grande quantidade de dados para que o modelo seja capaz de generalizar bem para novos casos. Para séries temporais, novos casos são as observações futuras. É importante também que os dados sejam representativos dos novos casos e que eles sejam de boa qualidade. Informações com erros de medidas, presença de valores discrepantes (chamados de outliers) ou cheio de ruídos, podem gerar previsões de baixa qualidade mesmo que o modelo utilizado seja adequado. Modelos podem realizar previsões imprecisas quando quando ele funciona muito bem para os dados de treinamento, mas não é capaz de prever com precisão novas informações. 2.1 O que pode ser previsto? 2.2 Principais modelos de previsão 2.3 O passo-a-passo de realizar uma previsão 2.4 Um Projeto de Previsão de Ponta a Ponta "],["características-de-séries-temporais.html", "Capítulo 3 Características de Séries Temporais 3.1 Objeto ts 3.2 Gráficos de Séries Temporais 3.3 Padrões das Séries Temporais 3.4 Decomposição", " Capítulo 3 Características de Séries Temporais Séries temporais se referem a dados observados em diferentes pontos no tempo. O uso de métodos estatísticos convencionais como o de regressão linear dependem e suposições de que as observações são independentes entre si, uma hipótese que não faz sentido quando se trabalha com observações no tempo: cada observação está correlacionada com as observações mais próximas no tempo. Assim, um choque no valor da ação que ocorre ontem tem efeitos no preço da ação hoje. Na economia observamos a taxa semanal de juros, o preço de fechamento das ações, o índice de preço mensal, as vendas anuais, e por ai vai. Em meteorologia, observamos as temperaturas médias diárias, a precipitação anual, índices de seca e outros. Na agricultura temos o registro anual da colheita e da produção de leite, erosão do solo e valor das exportações. A lista de áreas em que séries temporais podem ser estudadas não tem fim. O objetivo da análise de séries temporais são de: (1) entender ou modelar os mecanismos que produzem a série observada e (2) prever valores futuros de uma série com base na história da série, e quando possível, com relação a outras séries ou fatores. 3.1 Objeto ts Uma série temporal pode ser representada como uma lista de números em sequência, onde cada número é uma informação sobre um período específico de tempo. No R, essas informações podem ser registradas como objetos ts. A tabela 3.1 mostra um exemplo de dados anuais para o período entre 2012 e 2016. Table 3.1: Informações no Tempo Ano Observações 2012 123 2013 39 2014 78 2015 52 2016 110 Podemos transformar as informações acima em um objeto ts utilizando a função ts(): y &lt;- ts(c(123,39,78,52,110), start = 2012) y ## Time Series: ## Start = 2012 ## End = 2016 ## Frequency = 1 ## [1] 123 39 78 52 110 Se seus dados são anuais, com uma observação por ano, você precisa apenas fornecer o ano inicio (ou o ano final). Para observações que são mais frequentes que uma vez ao ano, é preciso fornecer um argumento de frequência (frequency). Por exemplo, se seus dados são mensais, então ele pode ser convertido para um objeto ts da seguinte maneira: dados_mensais &lt;- c(10, 30, 50, 70, 90) y &lt;- ts(dados_mensais, start = 2015, frequency = 12) y ## Jan Feb Mar Apr May ## 2015 10 30 50 70 90 3.1.1 Frequências de uma série temporal A frequência é o número de observações antes que o padrão sazonal se repita. Quando usamos a função ts no R, podemos utilizar as seguintes frequências: Table 3.2: Frequências de uma Série Temporal Dados Frequência Anual 1 Trimestral 4 Mensal 12 Semanal 52 Em relação ao padrão semanal, na verdade temos \\(365.25/7 = 52.18\\) semanas em média em um ano, e não 52 semanas. Contudo, a maioria das funções que utilizam a função ts exigem que a frequency seja fornecida como um número inteiro. Se a frequência de observações é maior do que uma vez por semana, existem algumas formas de lidar com a situação. Por exemplo, dados diários podem ter um padrão de sazonalidade semanal (frequency = 7) ou uma sazonalidade anual (frequency = 365.25). De modo similar, dados que são observados por minuto, podem ter um padrão sazonal a cada hora (frequency = 60), uma sazonalidade diária (frequency = 1440, ou \\(24 \\times 60\\)), uma sazonalidade semanal (frequency = 10080, ou \\(24 \\times 60 \\times 7\\)) e anual (frequency = 525960, ou \\(24 \\times 60 \\times 365.25\\)). Para utilizar um objeto ts nestes casos, é preciso decidir que desses padrões sazonais é mais importante. Contudo, o R permite lidar com dados com múltiplas sazonalidades utilizando técnicas mais avançadas. 3.2 Gráficos de Séries Temporais Para séries temporais, o gráfico apropriado mostra as observações no tempo, com observações consecutivas sendo unidas por uma linha reta. A figura 3.1 mostra um exemplo de um gráfico de séries temporais. library(fpp) ## Warning: package &#39;fpp&#39; was built under R version 4.1.2 ## Carregando pacotes exigidos: forecast ## Warning: package &#39;forecast&#39; was built under R version 4.1.1 ## Registered S3 method overwritten by &#39;quantmod&#39;: ## method from ## as.zoo.data.frame zoo ## Carregando pacotes exigidos: fma ## Warning: package &#39;fma&#39; was built under R version 4.1.2 ## Carregando pacotes exigidos: expsmooth ## Warning: package &#39;expsmooth&#39; was built under R version 4.1.2 ## Carregando pacotes exigidos: lmtest ## Warning: package &#39;lmtest&#39; was built under R version 4.1.1 ## Carregando pacotes exigidos: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric ## Carregando pacotes exigidos: tseries ## Warning: package &#39;tseries&#39; was built under R version 4.1.1 library(tidyverse) ## Warning: package &#39;tidyverse&#39; was built under R version 4.1.1 ## -- Attaching packages --------------------------------------- tidyverse 1.3.1 -- ## v ggplot2 3.3.5 v purrr 0.3.4 ## v tibble 3.1.5 v dplyr 1.0.7 ## v tidyr 1.1.4 v stringr 1.4.0 ## v readr 1.4.0 v forcats 0.5.1 ## Warning: package &#39;tibble&#39; was built under R version 4.1.1 ## Warning: package &#39;tidyr&#39; was built under R version 4.1.1 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() autoplot(melsyd[,&quot;Economy.Class&quot;]) + ggtitle(&quot;Passageiros da Classe Econômica: Melborne-Sydney&quot;) + xlab(&quot;Ano&quot;) + ylab(&quot;Milhares&quot;) Figure 3.1: Total de passageiros semanais na Compainha Áerea Ansett Os dados fazem parte do pacote fpp, que acompanha o livro Forecasting: Principles and Practice. Com a função autoplot(), produzimos gráficos no tempo a partir de objetos ts. Alternativamente podemos transformar o objeto ts em um data.frame: dados_passageiros &lt;- data.frame(passageiros_economicos = as.matrix(melsyd[,&quot;Economy.Class&quot;]), date = time(melsyd)) dados_passageiros %&gt;% head() %&gt;% knitr::kable() passageiros_economicos date 20.167 1987.481 20.161 1987.500 19.993 1987.519 20.986 1987.538 20.497 1987.558 20.770 1987.577 A partir do objeto dados_passageiros é possível produzir o mesmo gráfico utilizando o pacote ggplot2 (figura 3.2). dados_passageiros %&gt;% ggplot(aes(y = passageiros_economicos, x = date)) + geom_line() + labs(y = &quot;Milhares&quot;, x = &quot;Ano&quot;, title = &quot;Passageiros da Classe Econômica: Melborne-Sydney&quot;) Figure 3.2: Total de passageiros semanais na Compainha Áerea Ansett Independemente da forma como o gráfico é produzido, ele revela algumas características interessantes: Em 1989 nenhum passageiro foi transportado. Durante 1992 houve um período de redução de passageiros. Um grande aumento no número de passageiros transportados em 1991. Inícios de ano sempre produzem aumentos no numero de passageiros em razão do efeito das festas de fim de ano. Existe uma flutuação de longo prazo, em que o nível de passageiros é elevado no início da série, diminui em 1989, e volta a aumentar novamente entre 1990 e 1991. Existem alguns períodos sem informação. Para realizar previsão dessa série, todas essas características precisam ser levadas em conta. 3.2.1 Convertendo tk para data.frame O pacote timetk possui uma série de funções convenientes para converter objetos ts para outros formatos. A função tk_tbl converte o objeto ts para um objeto tibble (objeto equivalente a um dataframe). Já a função tk_ts faz o caminho inverso. library(timetk) ## Warning: package &#39;timetk&#39; was built under R version 4.1.1 ## Registered S3 method overwritten by &#39;tune&#39;: ## method from ## required_pkgs.model_spec parsnip melsyd %&gt;% # converte ts em tibble tk_tbl() %&gt;% mutate(index = as.Date(index)) %&gt;% tail() %&gt;% knitr::kable() index First.Class Business.Class Economy.Class 1975-06-16 1.417 3.152 27.322 1975-06-16 1.458 3.053 28.837 1975-06-16 1.398 2.745 26.548 1975-06-16 1.423 3.156 27.279 1975-06-16 1.358 3.069 27.306 1975-06-16 1.488 3.379 28.299 3.3 Padrões das Séries Temporais Ao descrever séries temporais, usamos palavras como tendência, sazonalidade e padrão cíclico. Antes disso, podemos observar uma série sem padrão definido. 3.3.1 Série sem padrão Na figura 3.3 vemos a média de precipitação na cidade americana de Los Angelos durante o período de 1878 a 1992. Estes dados estão no pacote TSA, que acompanha o livro Time Series Analysis. library(TSA) data(larain) larain &lt;- data.frame(chuva_anual = as.matrix(larain), date = time(larain)) larain %&gt;% ggplot(aes(x = date, y = chuva_anual)) + geom_line() + geom_point() + labs(x = &quot;&quot;, y = &quot;Precipitação em polegadas&quot;, title = &quot;Chuva Anual em Los Angeles, 1878 - 1992&quot;, subtitle = &quot;Em polegadas de Chuva por Ano&quot;) Figure 3.3: Padrão de Chuvas em Los Angeles É possível perceber uma variação no volume chuvas ao longo dos anos  alguns anos com um volume de chuva mais alto, outros mais baixo. Estamos interessados se os valores da chuva estão relacionados entre si. Será que ao conhecer o comportamento da chuva em um conjunto de anos podemos prever o que ocorrerá em anos consecutivos? Podemos criar uma variável relativa a chuva do ano anterior e compara-la com a chuva no ano corrente. A tabela 3.3 mostra essa comparação. larain &lt;- larain %&gt;% mutate(chuva_ano_anterior = lag(chuva_anual)) larain %&gt;% head() %&gt;% knitr::kable(caption = &quot;Volume Anual de Chuvas em Los Angeles&quot;) Table 3.3: Volume Anual de Chuvas em Los Angeles chuva_anual date chuva_ano_anterior 20.86 1878 NA 17.41 1879 20.86 18.65 1880 17.41 5.53 1881 18.65 10.74 1882 5.53 14.14 1883 10.74 Assim, em 1877 observamos 17,41 polegadas de chuva, e a chuva do ano anterior somou 20,86 polegadsa de chuva. Na figura 3.4 exibimos as duas informações lado a lado com um gráfico de dispersão. larain %&gt;% ggplot(aes(x = chuva_anual, y = chuva_ano_anterior)) + geom_point() + geom_smooth() + labs(x = &quot;Chuva anual&quot;, y = &quot;Volume de chuva do ano anterior&quot;, title = &quot;Volume de Chuva em Los Angeles, 1876 - 1992&quot;, subtitle = &quot;Relação entre a Chuva Anual e o Volume de Chuva no Ano Anterior&quot;) Figure 3.4: Relação entre chuva atual e o volume de chuva do ano anterior É possível observar que é perfeitamente possível que um ano com um elevado volume de chuvas seja seguido por um ano com baixo volume, e vice-versa. Assim, o volume de chuva que ocorre em um ano não parece explicar muito bem o comportamento da chuva nos anos seguintes. Essa série não parece ter nenhum tipo de tendência, que indicaria que o volume de chuva está aumentando ou diminuindo ao longo do tempo. Não parece existir correlação entre a chuva de um ano e do ano anterior, de modo que podemos ter um ano com um volume muito alto de chuva e no ano seguinte, um volume baixo. De um ponto de vista de modelagem e de previsão, está não é uma série muito interessante. A figura 3.5 mostra uma história diferente. data(hare) hare_df &lt;- data.frame(numero_coelhos = as.matrix(hare), data = time(hare)) hare_df %&gt;% ggplot(aes(x = data, y = numero_coelhos)) + geom_line() + geom_point() + labs(x = &quot;Ano&quot;, y = &quot;Ambundância&quot;, title = &quot;Ambundância de Coelhos Canadenses, 1905-1934&quot;) Figure 3.5: Ambundância de Coelhos Canadenses Diferentemente da série anterior, as observações de um ano estão correlacionadas de maneira muito próxima com aquelas observadas nos anos anteriores. Um número grande em um ano é seguido de um número semelhante no ano seguinte. Vendas de drogas anti-diabetes (figura 3.7) mostram sazonalidade que é induzida por mudanças nos preços dos medicamentos no final do ano calendário. 3.3.2 Sazonalidade A figura 3.6 mostra uma série temporal de temperatura média registrada para uma cidade no estado americano do Iowa. data(tempdub) tempdub &lt;- data.frame(temperatura = as.matrix(tempdub), data = time(tempdub)) tempdub %&gt;% ggplot(aes(x = data, y = temperatura)) + geom_line() + geom_point() + labs(x = &quot;Ano&quot;, y = &quot;Temperatura&quot;, title = &quot;Temperatura Média Mensal no Estado do Iowa, 1964-1976&quot;) Figure 3.6: Temperatura média anual em Iowa Nela observamos uma movimento sazonal muito regular. Um padrão sazonal ocorre quando uma série é afetada por fatores sazonais como o tempo do ano ou o dia da semana. Sazonalidade é sempre uma de uma frequência conhecida e fixa. Sazonalidade para valores mensais ocorre quando observações que são separadas por 12 meses estão relacionados de alguma maneira. Assim, as temperaturas em janeiro de 1910 estão relacionadas com as temperaturas de janeiro de 1911, 1912 e assim por diante. Todos os meses de janeiro e fevereiro são muito frios e são similares em valores de temperatura. Isto não quer dizer que todos os meses de janeiro terão a mesma temperatura, ou os meses de Junho terão sempre a mesma temperatura média. Assim, se quisermos encontrar um modelo para este tipo de série temos que acomodar estas variações mas preservar estas similaridades entre temperaturas dos mesmos meses. a10_df &lt;- data.frame(vendas_drogas = matrix(a10), date = time(a10)) a10_df %&gt;% ggplot(aes(x = date, y = vendas_drogas)) + geom_line() + labs(x = &quot;Ano&quot;, y = &quot;Milhões de US$&quot;, title = &quot;Vendas de Drogas Anti-diabéticos&quot;) Figure 3.7: Vendas de Medicamento contra diabetes A figura 3.8 mostra cada série anual de modo independente. ggseasonplot(a10, year.labels = T, year.labels.left = T) + ylab(&quot;Milhões de US$&quot;) + ggtitle(&quot;Gráfico Sazonal: Vendas de Drogas Anti-diabetes&quot;) Figure 3.8: Gráfico Sazonal de vendas de drogas anti-diabetes A figura permite observar o padrão sazonal com mais clareza, e é especialmente útil para entender o padrão interno dentro de um ano, assim como encontrar anos com mudanças de padrão. Especificamente observamos um aumento significativo da venda de drogas no início do ano. Podemos observar o mesmo gráfico com coordenadas polares. ggseasonplot(a10, polar = T) + ylab(&quot;Milhões de US$&quot;) + ggtitle(&quot;Gráfico Polar Sazonal: \\nVendas de Drogas Anti-diabetes&quot;) Figure 3.9: Gráfico sazonal polar das vendas mensais de medicamentos contra diabetes Outra figura que pode nos ajudar a identificar padrões sazonais é a produzida pela função ggsubseriesplot(). A figura 3.10 mostra um desses exemplos. ggsubseriesplot(a10) + ylab(&quot;Milhões de US$&quot;) + ggtitle(&quot;Sub-séries sazonais para vendas de medicamentos para diabetes&quot;) Figure 3.10: Sub-séries sazonais para vendas de medicamentos para diabetes A linha horizontal mostra a média para cada mês e nos permite enxergar o padrão sazonal com mais clareza. Novamente, ele consegue nos fazer enxergar que o mês de janeiro apresentar uma média muito elevada em comparação aos demais meses do ano. O pacote timetk também oferece uma série de funções para diagnóstico de padrões nas séries temporais. Contudo, o pacote exige que os dados fornecidos estejam no formato de data.frame. library(timetk) a10_df %&gt;% mutate(date = as.Date(date)) %&gt;% plot_seasonal_diagnostics(date, vendas_drogas, .interactive = F) (#fig:seasonal_timetk)Gráfico sazonal das vendas mensais de medicamentos contra diabetes 3.3.3 Tendência A série de vendas de drogas também exibe um padrão de tendência. Uma tendência ocorre quando existe um aumento (ou redução) de longo prazo nos dados. Ela não precisa ser linear, e muitas vezes pode apresentar mudanças de direção, que ocorrem quando uma tendência de aumento se converte em uma tendência de queda. A figura 3.11 mostra outra série com padrão forte de tendência. autoplot(fpp::cafe) + ggtitle(&quot;Gastos Trimestrais com consumo fora do domicílio, Australia&quot;) + xlab(&quot;Ano&quot;) + ylab(&quot;Milhões&quot;) Figure 3.11: Gastos com consumo fora do domicílio na Australia Os gastos dos australianos com consumo fora do domicílio (cafés e restaurantes) tem uma tendência crescente durante todo o período. Ela também exibe um comportamento sazonal dentro de um mesmo ano. 3.3.4 Padrão Cíclico Um comportamento cíclico surge quando os dados exibem um padrão de altas e quedas não possuem frequência fixa. Estas flutuações são os resultados, quase sempre, de condições econômicas, e estão relacionadas ao ciclo de negócios. A duração dessas flutuações tendem a ser de no mínimo 2 anos. A fabricação de equipamentos elétricos na zona do Euro (figura 3.12 exibe um padrão sazonal dentro de cada ano, mas também um forte comportamento cíclico com um período de cerca 5 anos ou mais. autoplot(fpp::elecequip) + ggtitle(&quot;Equipamentos Elétricos Produzidos na Área do Euro&quot;) + xlab(&quot;Ano&quot;) + ylab(&quot;Índice 2005 = 100&quot;) Figure 3.12: Equipamentos elétricos produzidos na zona do Euro Muitas pessoas confundem o comportamento sazonal, mas eles são bem diferentes. Se a flutuação não tem frequência fixa então elas são ciclos; se a frequência não se altera e está associada com algum aspecto do calendario, então o padrão é sazonal. Em geral, o tamanho médio dos ciclos é maior que o comprimento de um padrão sazonal, e a magnitude dos ciclos tende a variar mais do que a magnitude do padrão sazonal. 3.4 Decomposição working in progress "],["modelos-estatísticos.html", "Capítulo 4 Modelos Estatísticos 4.1 Ruído Branco 4.2 Média Móvel e Filtro 4.3 Autoregressões 4.4 Random Walk com Drift 4.5 Sinal no Ruído", " Capítulo 4 Modelos Estatísticos O objetivo principal da análise de séries temporais é desenvolver modelos matemáticos que ofereçam uma descrição plausível do dado amostral. Nós vamos assumir que uma série temporal pode ser definida como uma coleção de variáveis aleatórias indexadas de acordo com a ordem que elas são obtidas no tempo. Por exemplo, podemos considerar uma série temporal como uma sequência de variáveis aleatórias \\(y_1, y_2, y_3, ...\\), onde a variável aleatória \\(y_1\\) denota o valor tomado de uma série no primeiro período do tempo, a variável \\(y_2\\) denota o valor para o segundo período no tempo, e assim em diante. Em geral, uma coleção de variáveis aleatórias, \\(\\{Y_t\\}\\), indexada por \\(t\\) é referida como um processo estocástico. 4.1 Ruído Branco Ruído branco é um tipo muito simples de série gerada como uma coleção de variáveis aleatoriamente não correlacionadas, \\(w_t\\), com média 0 e variância finita \\(\\sigma^2_w\\). A figura 4.1 mostra uma série desse tipo. ruido_branco = rnorm(200,0,1) df = data.frame(ruido_branco = as.matrix(ruido_branco), x = time(ruido_branco)) df %&gt;% ggplot(aes(x = x, y = ruido_branco)) + geom_line() + labs(x = &quot;Tempo&quot;, y = &quot;y&quot;, title = &quot;Série Temporal do Tipo Ruido Branco&quot;) Figure 4.1: Série Temporal do Tipo Ruído Branco 4.2 Média Móvel e Filtro Podemos substituir a série de ruído branco \\(w_t\\) por uma média móvel que suaviza a série. Por exemplo, considere substituir \\(w_t\\) por uma média dos seus valores atuais e dos seus valores vizinhos no passado e no futuro. Assim, deixe que: \\[v_t = \\frac{1}{3} (w_{t-1} + w_t + w_{t+1})\\] que faz com que a série tenha a aparência mostrada na figura 4.2. df &lt;- df %&gt;% mutate(media_movel = stats::filter(ruido_branco, sides = 2, filter = rep(1/3, 3))) df %&gt;% ggplot(aes(x = x, y = media_movel)) + geom_line() + labs(x = &quot;Tempo&quot;, y = &quot;y&quot;, title = &quot;Série Temporal de Média Móvel&quot;) Figure 4.2: Série temporal de Média Móvel Uma inspeção da série mostra uma versão mais suavizada do gráfico de ruído branco é produzida, refletindo o fato que oscilações grandes são menos aparentes. 4.3 Autoregressões Suponha que consideremos possamos calcular uma série \\(y\\) como uma equação de segunda-ordem da série de ruídos brancos \\(w_t\\): \\[y_t = y_{t-1} - 0.9y_{t-2} + w_t\\] para \\(t=1,2,...,500\\). A equação acima representa uma regressão ou previsão dos valores atuais \\(x_t\\) de uma série temporal como uma função dos dois valores passados da série, e portanto, o termo autoregressivo é sugerido para este modelo. Temos aqui um problema com os valores iniciais da série uma vez que as condições iniciais são importantes (\\(x_0\\) e \\(x_{-1}\\)). Mas, assumindo que temos estes valores, podemos gerar uma sequência de valores apenas substituindo estes valores iniciais na equação acima. O resultado é exibindo na figura 4.3. ruido_branco &lt;- rnorm(250,0,1) # as primeiras 50 observações são excluídas para evitar problemas de startup autoregressivo &lt;- stats::filter(ruido_branco, filter=c(1,-.9), method = &quot;recursive&quot;)[-(1:50)] df &lt;- df %&gt;% mutate(autoregressivo = autoregressivo) df %&gt;% ggplot(aes(x = x)) + geom_line(aes(y = autoregressivo)) + labs(x = &quot;Tempo&quot;, y = &quot;y&quot;, title = &quot;Autogressivo&quot;) Figure 4.3: Série Temporal Autoregressiva O modelo autoregressivo acima mostra um comportamento periódico. O modelo autoregressivo acima e suas generalizações podem ser utilizados como um modelo explicativo para muitos tipos de séries observadas. 4.4 Random Walk com Drift Um modelo para analisar tendências, como as vistas em temperaturas globais, é o random walk com drift. Intuitivamente, em cada período do tempo, a variável toma um passo independente para cima ou para baixo, por isso o termo random walk. A variável vai subir ou descer? A probabilidade dos dois eventos é igual. Uma analogia comumente utilizada é a de um bêbado caminhando em zig-zag pela rua enquanto tenta se mover em frente: o caminho que ele segue é uma caminhada aleatória ou random walk. Assim, o modelo de random walk pode ser definido pela equação: \\[y_t = \\delta + y_{t-1} + w_t\\] para \\(t = 1,2,...,...\\) com condições iniciais \\(y_0 = 0\\) e onde \\(w_t\\)é um ruido branco. A constante \\(\\delta\\) é chamada de drift, e quando \\(\\delta = 0\\), chamamos o modelo simplesmente de random walk. Quando \\(\\delta =0\\), o valor da série em \\(t\\) é o valor da variável no tempo \\(t-1\\) mais um movimento completamente aleatório determinado por \\(w_t\\). Note que podemos reescrever a equação do random walk com drift ao acumular a soma de vários ruídos brancos: \\[x_t = \\delta t + \\sum_{j=1}^t w_t\\] para \\(t = 1,2,...\\). Ou seja, a série é uma soma de passos erráticos. O random walk é um processo que fornece um bom modelo para fenômenos tão diversos quanto preço de ações ou a posição de particulas pequenas suspensas em um flúido (movimento Browniano). A figura 4.4 mostra 500 observações geradas a partir de um modelo com \\(\\delta= 0\\), \\(\\delta = 0.2\\) e \\(\\delta_w = 1\\). set.seed(154) ruido_branco = rnorm(200) random_walk = cumsum(ruido_branco) ruido_branco_drift = ruido_branco + 0.2 random_walk_drift = cumsum(ruido_branco_drift) df &lt;- df %&gt;% mutate(random_walk = random_walk) %&gt;% mutate(random_walk_drift = random_walk_drift) df %&gt;% ggplot(aes(x = x)) + geom_line(aes(y = random_walk, color = &quot;Random Walk&quot;)) + geom_line(aes(y = random_walk_drift, color = &quot;Random Walk com Drift&quot;)) + labs(x = &quot;Tempo&quot;, y = &quot;&quot;, title = &quot;Random Walk&quot;, subtitle = &quot;Random Walk sem Drift e Random Walk com Drift&quot;, color = &quot;Modelo:&quot;) Figure 4.4: Série Temporal com Random Walk 4.5 Sinal no Ruído Muitos modelos realistas para gerar séries temporais assumem um sinal com algum tipo de variação periódica que é contaminada pela adição de um ruído aleatório. Por exemplo, considere um modelo do tipo: \\[y_t = 2 \\cos \\left(2 \\pi \\frac{t + 15}{50}\\right) + w_t\\] para \\(t = 1,2,...,200\\), onde o primeiro termo é o sinal. Abaixo temos um modelo aditivo simples na forma de \\(y_t = s_t + w_t\\), onde \\(s_t\\) denota algum sinal desconhecido e \\(w_t\\) denota um ruído branco. O problema de detectar um sinal e então extrair \\(s_t\\) é de grande interesse. Em economia, o sinal pode ser uma tendência ou um componente sazonal da série. curva_onda = 2*cos(2*pi*1:200/50 + .6*pi) df &lt;- df %&gt;% mutate(curva_onda = curva_onda, curva_onda_ruido = curva_onda + ruido_branco, curva_onda_ruido_forte = curva_onda + 5*ruido_branco) p1 &lt;- df %&gt;% ggplot(aes(x = x, y = curva_onda)) + geom_line() + labs(title = &quot;Curva em Onda&quot;, y = &quot;&quot;, x = &quot;&quot;) p2 &lt;- df %&gt;% ggplot(aes(x = x, y = curva_onda_ruido)) + geom_line() + labs(title = &quot;Curva em Onda + Ruído&quot;, y = &quot;&quot;, x = &quot;&quot;) p3 &lt;- df %&gt;% ggplot(aes(x = x, y = curva_onda_ruido_forte)) + geom_line() + labs(title = &quot;Curva em Onda + Ruído Forte&quot;, y = &quot;&quot;, x = &quot;&quot;) gridExtra::grid.arrange(p1, p2, p3) Figure 4.5: Séries Temporais com diferentes sinais "],["conceitos-teóricos.html", "Capítulo 5 Conceitos Teóricos 5.1 Média, Variâncias e Covariâncias 5.2 Função Média 5.3 Função de Autocovariância 5.4 A Função de Autocorrelação (FAC)", " Capítulo 5 Conceitos Teóricos Aqui vamos descrever alguns conceitos fundamentais sobre a teoria de séries temporais. Em particular, entender o que é um processo estocástico, a média, a função de covariância, processos estocásticos e a função de autocorrelação. 5.1 Média, Variâncias e Covariâncias Agora vamos introduzir várias medidas teoricas utilizadas para descrever como séries temporais se comportam. Como é usual em estatística, a descrição completa da série envolve uma função de distribuição multivariada a amostra conjunta dos valores \\(y_1, y_2, ..., y_n\\), enquanto que uma descrição mais econômica pode ser obtida em termos das funções média e de autocorrelação. Como a correlação é uma característica essencial da análise de séries temporais, as medidas de descrição mais úteis são aquelas expressadas em termos função de autocorrelação e função de autovariância. Vamos discutir alguns conceitos importantes relacionados a todos os tipo de modelos estatísticos de série temporal. Especificamente podemos utilizar algumas medidas para descrever uma série temporal. 5.2 Função Média A função média descreve o valor esperado de uma série temporal. Assim, para um processo estocástico \\(\\{ Y_t\\}\\), a função média é definida como: \\[\\mu_t = E(Y_t)\\] para \\(t = 0,1,2,...\\). Assim, \\(\\mu_t\\) é o valor esperado do processo no tempo \\(t\\). Exemplo 1: Função Média de Média Móvel Para uma media móvel dada por \\(\\frac{1}{3}(w_{t-1} + w_t + w_{t+1})\\), seu valor esperado (função média) é igual a zero, \\(\\mu_y = 0\\). Assim, a função média ao redor de zero descreve bem o comportamento geral da média móvel. O mesmo pode ser dito do ruído branco. df %&gt;% ggplot(aes(x = x, y = ruido_branco)) + geom_line(aes(color = &quot;Ruído Branco&quot;)) + geom_hline(aes(yintercept = 0, color = &quot;Função Média&quot;), linetype = 2, size = 1.5) + labs(x = &quot;Tempo&quot;, y = &quot;&quot;, title = &quot;Média Móvel&quot;, subtitle = &quot;Função Média da Média Móvel é Zero&quot;, color = &quot;&quot;) + theme(legend.position = &quot;bottom&quot;) (#fig:ruido_media)Função Média de um ruído branco Exemplo 2: Função Média de Random Walk com Drift Para um random walk com drift qual a função média? Dado que este modelo é dado por \\(y_t = \\delta t + \\sum_{j=1}^t w_t\\), como \\(E(w_t) = 0\\) e como \\(\\delta\\) é uma constante, temos \\[\\mu(yt) = \\delta t\\] Assim, a função média de uma random walk com drift é uma linha reta com inclinação \\(\\delta\\). Uma comparação de uma random walk com drift e sua função média pode ser vista na figura 5.1. df %&gt;% # criar a função média da random walk mutate(funcao_media_random_walk = .2*x) %&gt;% ggplot(aes(x = x)) + geom_line(aes(y = random_walk_drift, color = &quot;Random Walk com Drift&quot;)) + geom_line(aes(y = funcao_media_random_walk, color = &quot;Função Média&quot;), linetype = 2, size = 1.5) + labs(x = &quot;Tempo&quot;, y = &quot;&quot;, title = &quot;Random Walk com Drift Delta = 0.2&quot;, subtitle = &quot;A função Média é uma linha Reta com Inclinação Delta = 0.2&quot;, color = &quot;Modelo:&quot;) Figure 5.1: Função média de uma série random walk com drift Exemplo 3: Função Média de um Sinal mais Ruído A função média para um modelo aditivo na forma \\(y_t = s_t + w_t\\) é obtido por: \\[\\mu_{yt} = E(y_t) = E[2 \\cos(2\\pi \\frac{2t + 15}{50}) + w_t]\\] Novamente, \\(E(w_t) = 0\\) e os demais termos são constantes, logo: \\[\\mu_{yt} = E(y_t) = 2 \\cos(2\\pi \\frac{2t + 15}{50})\\] A média móvel é apenas o sinal sem o ruído. df %&gt;% ggplot(aes(x = x)) + geom_line(aes(y = curva_onda, color = &quot;Função Média&quot;), size = 1.5, linetype = 2) + geom_line(aes(y = curva_onda_ruido, color = &quot;Sinal com Ruído&quot;)) + labs(title = &quot;Curva em Onda e Sua Função Média&quot;, subtitle = &quot;A função Média é o Sinal sem Ruído&quot;, color = &quot;&quot;) + theme(legend.position = &quot;bottom&quot;) (#fig:sinal_media)Função Média de uma série com sinal 5.3 Função de Autocovariância A função de autocovariância mostra a covariância de um processo consigo mesmo em dois pontos diferentes no tempo. Assim, para uma série mensal de preços, a função de autocovariância mostra a relação entre \\(y_{jan}\\) e \\(y_{fev}\\), que são os valores observados de preço para janeiro e fevereiro, respectivamente. A função de autocovariância \\(\\gamma_{t,s}\\) é definida como: \\[\\gamma_{t,s} = Cov(Y_t, Y_s)\\] para \\(t,s=0,1,2,...\\) Onde \\(Cov(Y_t,Y_s) = E[(Y_t - \\mu_t)(Y_s - \\mu_s)] = E[Y_t Y_s) - \\mu_t \\mu_s\\). Portanto, a autocovariância mede a dependência linear entre dois pontos na mesma série observadas em pontos diferentes. Séries que são muito suavizadas exibem funções de autocovariância que permanecem altas mesmo quando o \\(t\\) e o \\(s\\) estão muito longes entre si. Séries com muita agitação tendem a ter funções de autocovariância que são próximas de zero para valores muito distantes entre si. Lembre que se \\(s = t\\), ou seja, se estivermos comparando uma observação no tempo consigo mesmo, a autovariância se reduz ao valor de variância, porque \\[\\gamma_{y}(t,t) = E[(y_t - \\mu_t)^2] = \\text{Var}(x_t)\\] Exemplo 1: Autocovariância de Ruído Branco Para uma série temporal ruído branco \\(w_t\\) que tem \\(E(w_t)=0\\), temos que \\[\\gamma_w = Cov(w_s, w_t) = 0\\] para \\(s \\neq t\\). Assim, a relação linear entre duas observações é zero. O valor de uma observação num ponto no tempo não influencia em nada os valores observados nos demais pontos do tempo. Exemplo 2: Autocovariância de uma Média Móvel Considere nosso exemplo de uma média móvel. Assim, \\[\\gamma_v (s,t) = cov(v_s, v_t) = cov \\{ \\frac{1}{3}(w_{s-1} + w_s + w_{s+1}), \\frac{1}{3}(w_{t-1} + w_t + w_{t+1}) \\}\\] Quando \\(s = t\\), temos \\[\\gamma_v (s,t) = \\frac{1}{9}cov\\{(w_{t-1} + w_t + w_{t+1}), (w_{t-1} + w_t + w_{t+1}) \\}\\] \\[\\gamma_v (s,t) = \\frac{1}{9}\\[ cov(w_{t-1}, w_{t-1} + cov(w_{t} + w_t) + cov(w_{t+1} + w_{t+1}) \\]\\] \\[\\gamma_v (s,t) = \\frac{3}{9} \\sigma^2_w\\] O mesmo exercício pode ser feito para \\(s = t + 1\\), quando analisamos observações que estão distantes uma observação entre si (janeiro e março, por exemplo), onde \\(\\gamma_v (t + 1, t) = \\frac{2}{9}\\sigma^2_w\\). Quando as observaçãos estão separadas por dois períodos, \\(|s-t| = 2\\), temos \\(\\gamma_v(s,t) = \\frac{1}{9}\\sigma^2_w\\), e quando as observações estão separadas por mais de dois períodos, \\(|s-t| &gt; 2\\), temos \\(\\gamma_v(s,t) = 0\\). Assim, quando suavizamos um ruído branco utilizando uma média móvel, adicionamos um pouco de covariância, mas esta dependência linear se reduz com o aumento da separação entre os valores. Assim, a relação entre janeiro e fevereiro é mais forte que a relação entre janeiro e março. Contudo, a relação entre janeiro e abril seria igual a zero, dado que \\(| \\text{Jan} - \\text{Abr} | &gt; 2\\). Exemplo 3: Autocovariância de um Random Walk Para um modelo random walk, \\(y_t = \\sum_{j = 1}^t w_t\\), temos que \\[\\gamma_y (s,t) = cov(y_s, y_t) = cov \\left( \\sum_{j=1}^2 w_j, \\sum_{k=1}^t w_t\\right) = \\min\\{s,t\\} \\sigma^2_w\\] porque o \\(w_t\\) são variáveis não correlacionadas entre si. Nota que, diferentemente dos exemplos anteriores, a função autocovariância de uma random walk depende no valor em particular de \\(s\\) e \\(t\\), e não na separação entre as observações ou no lag. Perceba também que a variãncia de uma random walk, \\(var(y_t) = t \\sigma^2_w\\), aumenta sem limites quando \\(t\\) cresce. O efeito desta variância pode ser observada na figura para o random walk. Conforme \\(t\\) aumenta, mais e mais a variável se distancia da sua função média \\(\\delta t\\). df %&gt;% # criar a função média da random walk mutate(funcao_media_random_walk = .2*x) %&gt;% ggplot(aes(x = x)) + geom_line(aes(y = random_walk_drift, color = &quot;Random Walk com Drift&quot;)) + geom_line(aes(y = funcao_media_random_walk, color = &quot;Função Média&quot;), linetype = 2, size = 1.5) + labs(x = &quot;Tempo&quot;, y = &quot;&quot;, title = &quot;Random Walk com Drift Delta = 0.2&quot;, subtitle = &quot;Quanto maior o t, maior a distância entre a Série e sua Função Média&quot;, color = &quot;Modelo:&quot;) + theme(legend.position = &quot;bottom&quot;) Figure 5.2: Autocovariância de uma random walk com drift 5.4 A Função de Autocorrelação (FAC) A correlação mensura a relação linear entre duas variáveis. A autocorrelação por sua vez mede a relação linear entre valores defasados ( lagged ) de uma série temporal. Uma forma de mensurar como um valor se relaciona a um valor passado é utilizando a Função de Autocorrelação (FAC), ou ACF em inglês. Ela mede a previsibilidade linear da série no tempo \\(t\\), usando apenas os valores de \\(y_s\\). A função de autocorrelação é definida como \\[\\rho (s,t) = \\frac{\\gamma(s,t)}{\\sqrt{\\gamma(s,s)\\gamma(t,t)}}\\] O valor de \\(\\rho\\), a covariância, esta sempre no intervalo \\([-1,1]\\). Se pudermos prever \\(y_t\\) perfeitamente a partir de \\(y_s\\) através de uma relação linear \\(y_t = \\beta_0 + \\beta_1 y_s\\), então a correlação será \\(+1\\) quando \\(\\beta_1 &gt; 0\\), e a correlação será \\(-1\\) quando \\(\\beta_1 &lt;0\\). Portanto, temos uma medida grosseira da nossa habilidade de prever a série no tempo \\(t\\) utilizando os valores no tempo \\(s\\). Vamos observar esse comportamento na prática utilizando os dados trimestrais de produção de cerveja. A figura 5.3 mostra a série. beer2 &lt;- window(ausbeer, start=1992) autoplot(beer2) + ggtitle(&quot;Produção trimestral de Cerveja, Australia&quot;) + ylab(&quot;&quot;) + xlab(&quot;Ano&quot;) Figure 5.3: Produção trimestral de cerveja É possível observar que os dados possuem um comportamento sazonal bem marcante. Vamos visualizar esse comportamento utilizando o correlograma, o gráfico que nos retorna os coeficientes de autocorrelação. library(forecast) ggAcf(beer2) Figure 5.4: Função de autocorrelação para a produção trimestral de cerveja O gráfico parece mostrar: O \\(r_4\\) é maior que outros lags. Isso é um resultado do padrão sazonal da série. Os picos tendem a ser separados por quatro trimestres e os vales tendem a ser separados por quatro trimestres. \\(r_2\\) é mais negativo que outros lags porque os vales e picos tendem a ser separados por dois trimestres. A linha azul indica se a correlação é significamente diferente de zero. 5.4.1 Tendência e sazonalidade em Correlogramas Quando os dados possuem uma tendência, as autocorrelações para pequenas defasagens é grande e positiva porque as observações próximas no tempo possuem valores semelhantes. Assim, o ACF de uma série com tendência tende a ter valores positivos que lentamente decaem conforme o número de valores defasados (lags) aumenta. A demanda por energia elétrica na australia (figura 5.5) possui um misto de padrão sazonal e tendência. aelec &lt;- window(elec, start=1980) autoplot(aelec) + xlab(&quot;Year&quot;) + ylab(&quot;GWh&quot;) Figure 5.5: Demanda por Energia Elétrica na Australia, 1980-1995 E a função de autocorrelação da série (figura 5.6) tem um decaimento lento devido a tendência, enquanto possui pequenas ondas, que ocorrem devido ao comportamento sazonal da série. ggAcf(aelec, lag=48) Figure 5.6: ACF da demanda australiana por energia elétrica O pacote timetk fornece uma função para gerar o correlograma. aelec_df &lt;- data.frame(electrical_demand = matrix(aelec), date = time(aelec)) %&gt;% mutate(date = as.Date(date)) aelec_df %&gt;% plot_acf_diagnostics(date, electrical_demand, .show_white_noise_bars = T, .interactive = F) ## Max lag exceeds data available. Using max lag: 187 A figura ilustra além do ACF, a função de autocorrelação parcial (PACF, em inglês) que será melhor explicada na seção xxx. 5.4.2 A Função de Covariância-Cruzada e de Correlação-Cruzada De maneira geral, gostariamos de medir a previsibilidade de uma outra série \\(y_t\\) a partir da série \\(x_s\\). Assumindo que ambas as séries tem variância finita, nós temos a seguinte definição para a função de covariância cruzada: \\[\\gamma_{x,y}(s,t) = cov(x_s, y_t) = E[(x_s - \\mu_{xs})]\\] A função de correlação cruzada (FCC) é dada por: \\[\\rho_{xy}(s,t) = \\frac{\\gamma_{xt}(s,t)}{\\sqrt{\\gamma_{x}(s,s) \\gamma_{y}(t,t)}}\\] Podemos inclusive extender a ideia acima para o caso de mais de duas séries. O pacote timetk oferece um banco de dados de vendas semanais na rede walmart. Além de informações de vendas semanais para diferentes departamentos de algumas lojas da Walmart. Além da série de tempo principal, temos algumas covariadas adicionais: isHoliday, que indica se a semana específica possui um feriado, Temperature, que indica a temperatura média da semana e Fuel_Price, que indica o preço do combustível naquela semana. walmart_sales_weekly %&gt;% head() %&gt;% knitr::kable(caption = &quot;Vendas semanais das lojas walmart&quot;) Table 5.1: Vendas semanais das lojas walmart id Store Dept Date Weekly_Sales IsHoliday Type Size Temperature Fuel_Price MarkDown1 MarkDown2 MarkDown3 MarkDown4 MarkDown5 CPI Unemployment 1_1 1 1 2010-02-05 24924.50 FALSE A 151315 42.31 2.572 NA NA NA NA NA 211.0964 8.106 1_1 1 1 2010-02-12 46039.49 TRUE A 151315 38.51 2.548 NA NA NA NA NA 211.2422 8.106 1_1 1 1 2010-02-19 41595.55 FALSE A 151315 39.93 2.514 NA NA NA NA NA 211.2891 8.106 1_1 1 1 2010-02-26 19403.54 FALSE A 151315 46.63 2.561 NA NA NA NA NA 211.3196 8.106 1_1 1 1 2010-03-05 21827.90 FALSE A 151315 46.50 2.625 NA NA NA NA NA 211.3501 8.106 1_1 1 1 2010-03-12 21043.39 FALSE A 151315 57.79 2.667 NA NA NA NA NA 211.3806 8.106 Na figura @ref(fig:walmart_11) exibimos a série de vendas no tempo apenas para o departamento 1 da loja 1. walmart_sales_weekly %&gt;% filter(id == &quot;1_1&quot;) %&gt;% plot_time_series(Date, Weekly_Sales) (#fig:walmart_11)Vendas semanais do Departamento 1 da Loja na Walmart Como temos acesso as covariadas Temperature e Fuel_Price podemos calcular não apenas o ACF, mas também a Função de Correlação-Cruzada com essas duas variáveis. Para tanto, vamos utilizar o parâmetro .ccf_vars na função plot_acf_diagnostics para incluir estas duas variáveis na análise. walmart_sales_weekly %&gt;% group_by(id) %&gt;% select(id, Date, Weekly_Sales, Temperature, Fuel_Price) %&gt;% plot_acf_diagnostics(Date, Weekly_Sales, # Calcular ACF &amp; PACF .ccf_vars = c(Temperature, Fuel_Price), # CCF .lags = &quot;3 months&quot;, .interactive = FALSE) Figure 5.7: Correlação Cruzada de Vendas Semanais com Temperatura e Preço de Combustível A função exibe o ACF e PACF de cada um dos departamentos de cada loja Walmart, além de calcular a correlação cruzada com temperatura e preço de combustíveis. Almas relações curiosas emergem, como a relação entre os valores de vendas semanais e valores passados de temperatura. É possível que essa relação esteja sendo intermediada pelo padrão sazonal de vendas mais fortes no fim do ano. É importante destacar que o padrão da correlação cruzada é afetado pela estrutura das duas séries e a tendência que cada uma tem. É preferível sempre remover a tendência das séries ou levar em consideração a estrutura do ARIMA univariado da variável \\(x\\) antes de aplicar um gráfico de CCF. Esses tópicos serão melhor discutidos na seção xxx e yyy. "],["considerações-práticas.html", "Capítulo 6 Considerações Práticas 6.1 Conjuntos de teste e treinamento 6.2 Transformação de variáveis 6.3 Rodando um modelo simples 6.4 Valores ajustados e resíduos 6.5 Medindo a performance da previsão 6.6 Validação Cruzada", " Capítulo 6 Considerações Práticas 6.1 Conjuntos de teste e treinamento A capacidade de um modelo de generalizar só pode ser realmente avaliada quando fazemos previsões de casos novos, para os quais o modelo não foi inicialmente treinado. Uma opção bastante utilizada é a de dividir os dados em dois subconjuntos: conjunto de treinamento e conjunto de teste. O modelo é treinado nos conjuntos de treinamento e testado em novos casos, no conjunto de teste. Podemos então comparar as previsões do modelo com os casos de teste e construir medidas de erro, que irão indicar se o modelo funciona bem para observações inéditas. A figura @ref(fig:teste_treinamento) mostra uma série temporal separada em bases de treinamento e teste. (#fig:teste_treinamento)Separação de uma série temporal em base de treinamento e de teste Na porção azul da figura, chamada de base de treinamento, o modelo de previsão é treinado. A partir desse modelo treinado, faremos previsões para o período de teste (vermelho), que são observações inéditas para o modelo, mas são informações conhecidas para o usuário. No exemplo acima, a base de teste é de 30% da série total. O tamanho da base de teste é usualmente de 20%. Um modelo que possui uma boa performance na própria base de treinamento não necessariamente será capaz de realizar boas previsões para casos inéditos. O modelo pode estar sobreajustando os dados de treinamento, e memorizando o comportamento do período de treinamento, mas sendo incapaz de generalizar bem. Afinal, é possível ajustar perfeitamente um conjunto de dados com um grande número de parâmetros. 6.1.1 Funções para dividir uma série temporal Na figura 6.1 podemos visualizar os dados de oferta semanal de gasolina para os EUA. library(tidymodels) # contém o pacote rsample library(fpp3) # contém os dados us_gasoline library(timetk) data(&quot;us_gasoline&quot;) us_gasoline &lt;- us_gasoline %&gt;% tk_tbl() %&gt;% mutate(Week = as.Date(Week)) ## Warning in tk_tbl.data.frame(.): Warning: No index to preserve. Object otherwise ## converted to tibble successfully. us_gasoline %&gt;% plot_time_series(Week, Barrels, .title = &quot;Oferta semanal de produtos de gasolina, EUA&quot;, .y_lab = &quot;Milhões de barril&quot;) Figure 6.1: Oferta semanal de produtos de gasolina, EUA No código acima, utilizamos o pacote rsample (parte do tidymodels), que contém uma série de pacotes para modelagem estatística. Carregamos o pacote fpp3, que contém a base de dados us_gasoline. Usamos ainda o pacote timetk, com algumas funções convenientes para tratamento de séries temporais. Utilizamos a função tk_tbl() do pacote timetk para converter o objeto ts para data.frame, e convertemos a coluna de data para um formato que possa ser reconhecido pelas demais funções do pacote timetk. Para separar os dados em base de treinamento e teste, vamos utilizar o pacote rsample. Com a função initial_split() podemos criar bases aleatórias de treinamento e teste. Para uma série temporal, como a estrutura temporal dos dados é importante, em vez de uma seleção aleatória, vamos definir bases de treinamento e teste por faixas dos dados. Especificamente, vamos definir que a base de treinamento corresponderá a 70% do total de dados, totalizando 948 observações. gasoline_split &lt;- us_gasoline %&gt;% initial_time_split(prop = 0.7) gasoline_split ## &lt;Analysis/Assess/Total&gt; ## &lt;948/407/1355&gt; O pacote rsample oferece duas funções adicionais: training() e testing, que retorna apenas as observações referentes aquela faixa específica. Podemos ainda visualizar como os dados foram separados utilizando as funções tk_time_series_cv_plan() e plot_time_series_cv_plan(), que preparam os dados e visualizam o plano de teste e treinamento (figura @ref(fig:treino_teste_gasolina)). df_treinamento &lt;- training(gasoline_split) df_teste &lt;- testing(gasoline_split) gasoline_split %&gt;% tk_time_series_cv_plan() %&gt;% plot_time_series_cv_plan(Week, Barrels, .title = &quot;Bases de Teste e Treinamento&quot;, .interactive = FALSE, #.line_alpha = 0.5, .line_type = 1, ) (#fig:treino_teste_gasolina)Bases de Teste e Treinamento para dados de gasolina, EUA 6.2 Transformação de variáveis Melhorias das previsões podem ser obtidas pela transformação das séries originais. Dados de total de vendas mensais podem ser afetados por efeitos calendários (meses com mais dias possuem naturalmente mais vendas totais). Uma forma de corrigir esta distorção é trabalhar com a média diária de vendas para cada mês. Dados agregados como produto interno bruto, total de empresas em uma cidade, total de leitos ou número de homicídios são distocidos por um efeito população. Uma transformação bastante popular é o de substituir a variável original por uma medida per capita. Quando trabalhamos com variáveis monetárias, ajustes pela inflação são fundamentais para realizar comparações justas ao longo do tempo. Para realizar este ajuste, é necessário utilizar um índice de inflação apropriado. Outros tipos incluem transformações matemáticas, como a mudança da série original para sua versão logarítima. Para uma série original \\(y_1,...,y_T\\), uma transformação logarítima tomará a forma \\(w_t = \\log(y_t)\\). Este tipo de transformação pode ser útil pela sua interpretabilidade, já que muadnças em um valor log são mudanças percentuais na escala original. Se o log na base 10 é utilizado, um aumento de 1 na escala log corresponde a multiplicar a variável original por 10. Contudo, se a variável original possui valores zero ou negativos, este tipo de transformação não é possível. 6.2.1 Funções para transformar dados Antes de aplicar qualquer modelo aos dados, podemos utilizar o pacote recipes (parte do pacote tidymodels) para criar pequenas receitas de bolo com instruções de pré-processamento dos dados. Ele torna simples tarefas como a criação de variáveis dummies, a normalização de variáveis numéricas, a criação de variáveis derivadas da coluna de tempo (como dummy de dia, mês, ano e dia da semana), além de muitas outras operações de feature engineering que são tão importantes, mas por vezes tediosas. E além, das transformações de variáveis, podemos ainda selecionar variáveis e remover colunas com variância zero. Trabalhando com a base de dados us_gasoline, vamos converter os dados originais para sua versão logarítima. Primeiro, com a função recipe podemos utilizar uma fórmula para indicar a variável dependente e as variáveis independentes. No caso de uma série temporal univariada, a coluna de tempo (Week) pode ser incluida como variável independente. Precisamos ainda indicar qual a base a ser transformada, que será a base de treinamento. receita_gasolina &lt;- us_gasoline %&gt;% recipe(Barrels ~ Week, training(gasoline_split)) receita_gasolina ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 1 Agora podemos realizar as transformações desejadas. Primeiro vamos utilizar a função step_log para criar uma versão log da variável dependente, que é selecionada a partir da função selecionadora all_outcomes (alternativamente podemos utilizar o nome da variável). receita_gasolina &lt;- receita_gasolina %&gt;% step_log(all_outcomes()) Na tabela @ref(tab:tab_gasolina_log) vemos como a função recipe transformou a coluna Barrels para formato log. receita_gasolina %&gt;% prep() %&gt;% juice() %&gt;% head() %&gt;% knitr::kable() Week Barrels 1991-02-04 1.890246 1991-02-11 1.861441 1991-02-18 1.884339 1991-02-25 1.977409 1991-03-04 1.927892 1991-03-11 1.938310 recipe ainda permite uma lista de outras transformações. Com step_meanimpute realizamos a imputação de valores faltantes utilizando a média da série. Transformação quadrática é possível com a função step_sqrt() e transformações box-cox é possível com step_BoxCox(). Podemos usar uma especificação genérica da função dplyr::mutate() com step_mutate(). Podemos ainda discretizar variáveis numéricas com step_cut(), gerar variáveis derivadas da data com step_date (dummies de dia, mês, ano, e dia da semana) e muitas outras. Uma lista de todas as transformações possíveis pode ser obtida na documentação do recipe. 6.3 Rodando um modelo simples Nas próximas seções veremos modelos mais complexos, e mais apropriados para os dados utilizados. Por enquanto, vamos utilizar alguns modelos simples, que podem servir como benchmarking para os modelos por vi. 6.3.1 Método da Média Para este modelo, a previsão de todos os valores futuros é igual a média dos dados históricos. Se deixarmos os dados históricos serem denotados por \\(y_1,...,y_T\\), então os valores futuros serão dados como \\[\\hat{y}_{T+h|T} = \\bar{y}=\\frac{y_1 + ... +y_T}{T}\\] A notação \\(\\hat{y}_{T+h|T}\\) pode ser entendido como estimação de \\(\\hat{y}_{T+h}\\) baseada nos dados de \\(y_1,...,y_T\\). 6.3.2 Método Naïve Para previsões naïve, fazemos a previsão baseada no valor da última observação. Assim, \\[\\hat{y}_{T+h|T} = y_T\\] A previsão naïve pode ser ótima quando temos dados de passeio aleatório. 6.3.3 Método Naïve Sazonal É um método útil para dados sazonais. Neste caso, cada valor previsto é igual ao último valor observado do mesmo período sazonal do ano (ou o mesmo mês do ano anterior). Formalmente, a previsão para o tempo \\(T+h\\) é escrita como \\[\\hat{y}_{T+h|T} = y_{T+h-m(k+1)}\\] onde \\(m=\\text{período sazonal}\\), e \\(k\\) é a parte inteira de \\((h-1)/m\\) (exemplo, o número de anos completos no período de previsão antes do tempo \\(T+h\\)). Assim, com dados mensais, a previsão para todos os meses de fevereiro serão iguais ao valor do último fevereiro observad. Com dados trimestrais, a previsão de todos os segundos trimestres serão iguais ao último segundo trimestre observado. 6.3.4 Função para ajustar modelos: parsnip Finalmente, após as etapas de pré-processamento dos dados e a criação de um conjunto de treinamento e teste podemos utilizar o pacote parsnip para estimar alguns modelos. O parsnip realiza um ótimo trabalho em unificar uma série de diferentes modelos estatísticos e de machine learning em um único ambiente. O pacote é extremamente conveniente porque permite que o usuário utilize uma única forma de se comunicar com diferentes modelos que inicialmente possuiam sintaxes totalmente diferentes ou exigiam dados em diferentes formatos (matrix, ts, data.frame). Para utilizar o parsnip, sempre começamos definindo o modelo. Assim, para estimar uma regressão linear utilizamos a função linear_reg() e para estimar um random forest utilizamos a função rand_forest(). Contudo, muitos outros modelos estão presentes, como o modelo ARIMA (arima_reg), o modelo prophet (prophet_reg), Support Vector Machines (svm_poly e svm_rbf), regressão logística (logistic_reg), KNN (nearest_neighbor) e muitos outros. Uma lista completa de todos os modelos suportados pode ser encontrada na documentação do parsnip. O pacote modeltime extende o total de modelos para incluir modelos exclusivos de séries temporais. Vamos utilizar ajustar os dados de log de gasolina para o modelo naïve e o modelo naïve sazonal. Primeiro vamos definir o modelo a ser ajustado com a função naive_reg(), e fixar o pacote R que contêm a função naive() original. modelo_snaive &lt;- naive_reg( seasonal_period = 52 ) %&gt;% set_engine(&quot;snaive&quot;) modelo_naive &lt;- naive_reg() %&gt;% set_engine(&quot;naive&quot;) Para o modelo Naïve Sazonal, precisamos fixar o parâmetro de período da sazonalidade. Como os dados de gasolina estão em frequência semanal, temos uma sazonalidade bem peculiar, dado que o período de sazonalidade é de em média \\(365,25/7=52,18\\). A maioria dos modelos sazonais só aceitam valores inteiros para a frequência, e mesmo um valor aproximado de 52 períodos pode gerar resultados inadequados. Independemente dessa falha conhecida, vamos utilizar um seasonal_period = 52 como parâmetro do naive_reg. 6.3.5 Ajustando os modelos Agora temos os três ingredientes mais importantes para nosso modelo preditivo: (1) temos as bases de treinamento e teste, (2) temos uma receita de bolo com o passo-a-passo do pré-processamento que deve ser aplicado em todas as bases de dados; e (3) declaramos o modelo que deve ser ajustado. Para facilitar a integração de todas essas peças, podemos utilizar o pacote workflows, parte da suite tidymodels. Iniciamos um workflow sempre com a função workflow(). Adicionamos o modelo definido acima com add_model() e a receita que deve ser aplicada aos dados com add_recipe(). O último passo é o ajuste do modelo, onde passamos a base de treinamento construída pelo pacote rsample para que estimar o modelo naïve e naïve sazonal. workflow_naive &lt;- workflow() %&gt;% add_recipe(receita_gasolina) %&gt;% add_model(modelo_naive) %&gt;% fit(training(gasoline_split)) workflow_snaive &lt;- workflow() %&gt;% add_recipe(receita_gasolina) %&gt;% add_model(modelo_snaive) %&gt;% fit(training(gasoline_split)) Para facilitar o trabalho quando temos diversos modelos, podemos criar uma tabela de modelos com a função modeltime_table(), parte do pacote modeltime. Nela incluimos os arquivos com os modelos ajustados (workflow_*). tbl_modelos &lt;- modeltime_table( workflow_snaive, workflow_naive ) Por fim, usamos a função modeltime_calibrate() para produzir os valores previstos (fitted) para o período de teste e o valor dos resíduos. Mais a frente vamos analisar melhor os resíduos, mas antes vamos observar como os dois modelos realizaram as previsões para a oferta de produtos de gasolina (figura 6.2). Para tanto usamos a função modeltime_forecast(), para preparar um data.frame com as previsões e os dados observados, e a função plot_modeltime_forecast() para produzir a visualização desejada. tbl_calibracao &lt;- tbl_modelos %&gt;% modeltime_calibrate(new_data = testing(gasoline_split)) tbl_calibracao %&gt;% modeltime_forecast(new_data = testing(gasoline_split), actual_data = us_gasoline) %&gt;% plot_modeltime_forecast() Figure 6.2: Previsão de métodos naïve para produtos de gasolina nos EUA A última observação da base de treinamento foi na semana do dia 30 de março de 2009. Naquele dia, o log do total de barris foi de 9,024. O método naïve simplesmente reproduziu este valor para todo o período de teste. Por isso vemos uma reta verde. O modelo naïve com sazonalidade é capaz de capturar o comportamento da série surpreendemente bem, uma vez que a série tem um comportamento sazonal bem marcante. 6.4 Valores ajustados e resíduos Cada observação na série temporal pode ser prevista usando todas as observações anteriores. Chamamos estas novas observações de valores ajustados (fitted values), e eles são denotados por \\(\\hat{y}_{t|t-1}\\), ou simplesmente \\(\\hat{y}_t\\). Já os resíduos são a diferença entre os valores observados e os valores ajustados do modelo: \\[e_t = y_t - \\hat{y}_t\\] Se os dados foram transformados, é sempre útil olhar os resíduos na escala transformada. Resíduos na escala transformada são chamados de resíduos da inovação, e podemos denotar por \\(w_t - \\hat{w}_t\\), dado que \\(w_t = \\log{y_t}\\). Podemos acessar os valores observados, previstos e os resíduos para os dois modelos ao observar a tabela de calibração gerada pela função modeltime_calibrate(). tbl_calibracao %&gt;% unnest(.calibration_data) %&gt;% head(10) ## # A tibble: 10 x 8 ## .model_id .model .model_desc .type Week .actual .prediction .residuals ## &lt;int&gt; &lt;list&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 &lt;workf~ SNAIVE [52] Test 2009-04-06 2.19 2.23 -0.0431 ## 2 1 &lt;workf~ SNAIVE [52] Test 2009-04-13 2.21 2.22 -0.00861 ## 3 1 &lt;workf~ SNAIVE [52] Test 2009-04-20 2.21 2.22 -0.00414 ## 4 1 &lt;workf~ SNAIVE [52] Test 2009-04-27 2.19 2.23 -0.0426 ## 5 1 &lt;workf~ SNAIVE [52] Test 2009-05-04 2.19 2.23 -0.0473 ## 6 1 &lt;workf~ SNAIVE [52] Test 2009-05-11 2.22 2.24 -0.0137 ## 7 1 &lt;workf~ SNAIVE [52] Test 2009-05-18 2.26 2.24 0.0175 ## 8 1 &lt;workf~ SNAIVE [52] Test 2009-05-25 2.20 2.21 -0.0120 ## 9 1 &lt;workf~ SNAIVE [52] Test 2009-06-01 2.21 2.24 -0.0291 ## 10 1 &lt;workf~ SNAIVE [52] Test 2009-06-08 2.24 2.22 0.0111 É sempre útil checar se um modelo capturou de modo adequado as informações dos dados. Para tanto podemos utilizar algumas ferramentas de diagnósticos de resíduos para verificar se: os resíduos são não correlacionados. Se existe correlação entre os resíduos, então existe informação presente nos resíduos que deveria ter sido utilizada para computar as previsões. Os resíduos devem ter média zero. Se a média é diferente de zero, então as previsões estão viesadas. Qualquer previsão que não satisfaz estas propriedades pode ser melhorado. Contudo, isto não significa que um modelo que satisfaça essas condições seja um modelo que não possa ser melhorado. Assim, checar essas informações é uma forma de garantir que o método está utilizando toda a informação dispoível, mas não é uma boa forma de selecionar o método ideal a ser escolhido. É útil (mas não necessário) que os resíduos também variância constante (a chamada homocedasticidade), e sejam normalmente distribuidos. Estas propriedades tornam os cálculos de intervalo de confiança mais precisos. Vamos utilizar a tabela de calibração para plotar os resíduos do modelo naïve sazonal. A figura 6.3 mostra os resíduos da previsão de gasolina utilizando o método naïve sazonal. tbl_calibracao %&gt;% unnest(.calibration_data) %&gt;% filter(.model_desc == &quot;SNAIVE [52]&quot;) %&gt;% plot_time_series(Week, .residuals, .title = &quot;Resíduos do Modelo Naïve Sazonal&quot;) Figure 6.3: Resíduos do Modelo Naïve Sazonal Os resíduos parecem ter média zero no início e fim do período de teste, mas entre os anos de 2011 e 2014, temos resíduos que não possuem média zero, o que pode ser o resultado de um comportamento diferente da média história que não está sendo capturado pelo modelo de previsão. Isto pode indicar que as previsões produzidas por este método possuem um viés. É a impressão que temos quando analisamos a figura 6.2). A previsão parece sempre superestimar o log da oferta de produtos de gasolina. Podemos ainda produzir o histograma dos resíduos do método naïve sazonal para verificar se os resíduos possuem uma distribuição que se assemelhe a uma normal. A figura 6.4 parece indicar que os resíduos não são exatamente normais. tbl_calibracao %&gt;% unnest(.calibration_data) %&gt;% filter(.model_desc == &quot;SNAIVE [52]&quot;) %&gt;% ggplot(aes(x = .residuals)) + geom_histogram() + labs(title = &quot;Histograma dos Resíduos de um método Naïve&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 6.4: Histograma dos Resíduos de um método Naïve Resíduos não normais podem indicar que as previsões a partir deste método podem até produzir resultados não viesados, mas os intervalos de confiança estimados podem ser imprecisos, uma vez que o seu cálculo assumem sempre distribuição normal. Por fim, podemos verificar se os erros são autocorrelacionados utilizando a função de autocorrelação apresentada na seção xxx. A figura 6.5 mostra o correlograma produzido pela função plog_acf_diagnostics(). tbl_calibracao %&gt;% unnest(.calibration_data) %&gt;% filter(.model_desc == &quot;SNAIVE [52]&quot;) %&gt;% plot_acf_diagnostics(Week, .residuals, .show_white_noise_bars = T, .lags = 100, .title = &quot;Correlograma dos resíduos do método naïve aplicado a série de gasolina&quot;) Figure 6.5: Correlograma dos resíduos do método naïve aplicado a série de gasolina O correlograma parece indicar que os resíduos produzidos pelo método naïve são autocorrelacionados e que o modelo pode ser melhorado. A autocorrelação dos resíduos pode ser testada formalmente a partir de alguns testes estatísticos como o teste de Ljung-Box e o teste de Box-Pierce. 6.4.1 Teste de Ljung-Box O gráfico da Função de Autocorrelação \\(r_k\\) mostra a autocorrelação para cada lag \\(k\\), e realiza um teste de hipótese que avalia se a autocorrelação \\(r_k\\) é estatísticamente diferente daquilo que se considera um ruido branco. Cada uma destes testes de hipótese carrega a possibilidade de produzir falsos positivos, de modo que alguns valores de autocorrelações moderados, podem ser confundidos com um resíduo com autocorrelação remanescente, mas quando tomamos em conjunto parecem excessivos. Assim, é possível testar se os primeiros \\(K\\) autocorrelações são significativamente diferentes de um ruído branco. Um teste para um grupo de autocorrelações foi proposto por Box and Pierce (1970). Este teste é baseado na estatística \\[Q = n(\\hat{r}^2_1 + \\hat{r}^2_2 + ... + \\hat{r}^2_K)\\] onde \\(K\\) é o número máximo de lags sendo considerados e \\(n\\) é o número de observações. Se cada \\(r_k\\) é próxima de zero, então \\(Q\\) será pequeno. Se alguns dos valores de \\(r_k\\) são grandes, então \\(Q\\) será grande. Hyndman and Athanasopoulos (2018) sugere um valor de \\(K=10\\) para dados não-sazonais, e de \\(K=2m\\), para dados sazonais, onde \\(m\\) é o período de sazonalidade. Para um valor elevado de \\(n\\), \\(Q\\) tem uma distribuição chi-quadrado, mas segundo Ljung and Box (1978), mesmo valores de \\(n = 100\\) pode produzir aproximações não satisfatórias. Eles propõem uma versão modificada do teste de Box-Pierce, chamada de teste Ljung-Box, que define uma estatística de teste cuja distribuição nula é muito mais próxima da distribuição chi-quadrado. A estatística é dada por \\[Q_* = n(n+2)\\left( \\frac{\\hat{r}^2_1}{n-1} + \\frac{\\hat{r}^2_2}{n-2} + ... + \\frac{\\hat{r}^2_K}{n-K} \\right)\\] Novamente, valores elevados de \\(Q_*\\) sugerem que a autocorrelação não é um produto de ruído branco. 6.4.2 Função para A função modeltime_residuals_test() toma os resíduos de cada modelo guardado na tabela de calibração tbl_calibracao e retorna uma série de testes estatísticos para os resíduos. A tabela 6.1 mostra os resultados dos testes de resíduos produzidos por esta função. tbl_calibracao %&gt;% modeltime_residuals() %&gt;% modeltime_residuals_test() %&gt;% knitr::kable(caption = &quot;Testes estatísticos para os resíduos&quot;) Table 6.1: Testes estatísticos para os resíduos .model_id .model_desc shapiro_wilk box_pierce ljung_box durbin_watson 1 SNAIVE [52] 0.0149333 0 0 0.6583350 2 NAIVE 0.0025252 0 0 0.5446369 Além dos p-valores dos testes de Pierce-Box e Ljung-Box para autocorrelação, que não rejeitam a presença de autocorrelação, a tabela ainda mostra o resultado para o teste de Durbin-Watson e o p-valor para o teste de Shapiro-Wilks. O teste de Durbin-Watson também testa a hipótese de autocorrelação, e parece indicar a presença de autocorrelação positiva (valores entre 0 e &lt;2). O teste de Shapiro-Wilks testa mais rigorosamente a hipótese de normalidade dos resíduos, com uma hipótese nula de que os resíduos são normalmente distribuidos. O teste funciona ao calcular a correlação entre os resíduos e os quantis normais. Quanto menor esta correlação, maior a evidência de normalidade. Analisando a tabela, ao nível de significância de 0,05, nossos resíduos não parecem ser normalmente distribuidos, confirmando nossa inspeção visual do histograma dos resíduos. 6.5 Medindo a performance da previsão O erro de previsão é medido como a diferença entre o valor observado e o previsto e pode ser escrito como \\[e_{T+h} = y_{T+h} - \\hat{y}_{T+h|T}\\] onde os dados de treinamento são dados por \\(\\{y_1, y_2, ..., y_T\\}\\) e dados de teste são dados por \\(\\{y_{T+1}, y_{T+2},...\\}\\). Lembrando que os erros de previsão são a diferença entre dados observados e previstos para o período de teste, enquanto resíduos são a diferença entre dados observados e ajustados no período de treinamento. 6.5.1 Erros dependentes da escala Algumas medidas de erro de previsão são dependentes da escala dos dados, não sendo indicados para comparação entre previsões feitas para séries temporais em escalas diferentes. As medidas de erro dependente de escala mais comuns são o Erro Médio Absoluto (MAE, em inglês) e o Raiz quadrada do Erro Quadrado Médio (RMSE, em inglês): \\[\\text{MAE} = \\text{média}(|e_t|)\\] \\[\\text{RMSE} = \\sqrt{\\text{média}(e_t^2)}\\] 6.5.2 Erros de porcentagem Erros de porcentagem não dependem da escala da série temporal, sendo utilizando para comparar performance de previsão de séries temporais de escalas diferentes. A médida mais utilizada é o Erro percentual da Média Absoluta (MAPE, em inglês). Esta medida é dada pela fórmula \\[\\text{MAPE} = \\text{média}(|p_t|)\\] onde \\(p_t = 100 e_t / y_t\\). A principal desvantagem deste tipo de medida é apresentar valores infinitos quando o valor de \\(y_t = 0\\) e valores extremos quando \\(y_t \\rightarrow 0\\). Este tipo de médida também é assimétrica, na medida que penaliza mais erros negativos em detrimento de erros positivos. Uma medida percentual alternativa é a MAPE simétrica ou sMAPE, que é definida como \\[\\text{sMAPE} = \\text{média} \\left(200 \\frac{|y_t - \\hat{y}_t|}{y_t + \\hat{y}_t}\\right)\\] Contudo, se \\(y_t\\) e \\(\\hat{y}_t\\) são próximos de zero, a medida também envolve uma fração com denominador próximo de zero. 6.5.3 Erros Escalados Como uma alternativa aos erros de percentagem, Hyndman and Koehler (2006) propõem reescalar os erros baseados na medida de MAE dos dados de treinamento de um método de previsão simples. Para uma série temporal sem sazonalidade, uma forma de definir o erro de escala utiliza uma previsão naïve: \\[q_j = \\frac{e_j}{\\frac{1}{T-1} \\sum_{t=2}^T |y_t - y_{t-1}|}\\] Como o numerador e o denominador envolvem valores que estão na escala dos dados originais, \\(q_j\\) é independente da escala dos dados. \\(q_j &lt; 1\\): \\(q_j\\) é produzido por uma previsão melhor que a média de previsão um passo-a-frente do modelo naïve computado nos dados de treinamente. \\(q_j &gt; 1\\) se a previsão é pior. Para dados com sazonalidade, o erro escalado pode ser definido ao utilizar uma previsão naïve com sazonalidade. \\[q_j = \\frac{e_j}{\\frac{1}{T-m} \\sum_{t=2}^T |y_t - y_{t-m}|}\\] Assim, o Erro Escalado Médio Absoluto (Mean Absolute Scaled Error, MASE) é dado por \\[\\text{MASE} = \\text{média}(|q_j|)\\] Alternativamente podemos calcular o Root Mean Squared Scaled Error (RMSSE) como \\[\\text{RMSSE} = \\sqrt{\\text{média}(q_j^2)}\\] 6.5.4 Função para Medidas de Performance O pacote yardstick facilita a criação de medidas de performance dos modelos estimados, produzindo as principais medidas de desempenho para problemas de regressão (RMSE, R-Quadrado e outros) e de classificação (matriz de confusão, precisão, acurácia e outros). O pacote modeltime nos fornece a função modeltime_accuracy(), que permite utilizar as funções do yardstick para objetos do tipo workflow. A tabela 6.2 mostra as medidas de desempenho de previsão para os dois modelos estimados. Para tanto, passamos a tabela de calibração para a função modeltime_accuracy(). tbl_calibracao %&gt;% modeltime_accuracy(new_data = testing(gasoline_split)) %&gt;% #select(.model_desc, mape, rmse) %&gt;% kableExtra::kable(caption = &quot;Medidas de Desempenho para Previsão Naïve e Naïve com Sazonalidade&quot;) ## Warning in cor(truth, estimate): o desvio padrão é zero Table 6.2: Medidas de Desempenho para Previsão Naïve e Naïve com Sazonalidade .model_id .model_desc .type mae mape mase smape rmse rsq 1 SNAIVE [52] Test 0.0345147 1.585994 1.415764 1.573491 0.0431112 0.1741282 2 NAIVE Test 0.0345067 1.582057 1.415437 1.574987 0.0437255 NA Considerando que estamos comparando modelos diferentes aplicados aos mesmos dados, a questão de escala não é relevante. Da mesma forma, independentemente da medida utilizada, os dois modelos parecem produzir resultados semelhantes. 6.6 Validação Cruzada cross_validation &lt;- time_series_cv( data = us_gasoline, assess = &quot;5 years&quot;, initial = &quot;10 years&quot;, skip = &quot;3 years&quot;, slice_limit = 8 ) ## Using date_var: Week cross_validation %&gt;% tk_time_series_cv_plan() %&gt;% plot_time_series_cv_plan(Week, Barrels, .facet_ncol = 2, .interactive = FALSE) library(modeltime.resample) ## Warning: package &#39;modeltime.resample&#39; was built under R version 4.1.2 ajustes_reamostragens &lt;- tbl_modelos %&gt;% modeltime_fit_resamples( resamples = cross_validation ) ajustes_reamostragens %&gt;% plot_modeltime_resamples( .point_size = 3, .point_alpha = 0.8, .interactive = FALSE ) ## Warning in cor(truth, estimate): o desvio padrão é zero ## Warning in cor(truth, estimate): o desvio padrão é zero ## Warning in cor(truth, estimate): o desvio padrão é zero ## Warning in cor(truth, estimate): o desvio padrão é zero ## Warning: Removed 4 rows containing missing values (geom_point). ## Warning: Removed 4 rows containing missing values (geom_vline). Figure 6.6: Medidas de Performance para cada reamostragens ajustes_reamostragens %&gt;% modeltime_resample_accuracy(summary_fns = median) %&gt;% table_modeltime_accuracy() ## Warning in cor(truth, estimate): o desvio padrão é zero ## Warning in cor(truth, estimate): o desvio padrão é zero ## Warning in cor(truth, estimate): o desvio padrão é zero ## Warning in cor(truth, estimate): o desvio padrão é zero References "],["modelo-de-regressão-para-séries-temporais.html", "Capítulo 7 Modelo de Regressão para Séries Temporais", " Capítulo 7 Modelo de Regressão para Séries Temporais working in progress "],["modelos-arima.html", "Capítulo 8 Modelos ARIMA", " Capítulo 8 Modelos ARIMA "],["modelos-de-regressão-dinâmicos.html", "Capítulo 9 Modelos de Regressão Dinâmicos 9.1 Estimação 9.2 Regressão com Erros ARIMA", " Capítulo 9 Modelos de Regressão Dinâmicos O uso de variáveis explicativas exógenas é uma maneira óbvia de melhorar a precisão das precisões. Em vez de depender apenas de informações históricas sobre a série em si, podemos utilizar outras informações relevantes. Modelos de séries temporais como ARIMA permitem que valores da série sejam previstos a partir da inclusão de informações do passado, mas não permitem a inclusão destas variáveis exógenas relevantes como dummies de feriado, atividade dos concorrentes, mudanças nas leis, variáveis macroeconômicas e outras covariadas externas que podem ajudar a explicar a variação histórica de uma série temporal. Já modelos de regressão permitem a inclusão de variáveis externas, mas não são capazes de modelar as dinâmicas presentes em séries temporais, como os modelos ARIMA são capazes. 9.0.1 Valor de utilizar variáveis explicativas Variáveis externas podem ser especialmente úteis para previsão de demanda por eletricidade, que é altamente dependente da temperatura ambiente, uma vez que dias quentes levam a maior uso de ar condicionados (Taieb and Hyndman 2014). Contudo, nem sempre variáveis externas podem ser tão úteis. No caso de temperatura como uma variável explicativa para demanda por eletricidade, as previsões metereológicas podem fornecer medidas bem precisas do comportamento futuro, mas em casos onde as previsões das variáveis explicativas são imprecisas, adiciona-las ao modelo pode produzir resultados inconsistentes. Outro problema pode ocorrer caso a relação entre \\(y\\) e \\(x\\) é um fato histórico, mas pode não se repetir no futuro. Ou quando duas variáveis possuem uma relação positiva em um período, e negativa em outro. Esse tipo de problema pode produzir modelos com má especificações e previsões imprecisas. Assim, antes de recorrer a variáveis externas em modelos de séries temporais, é sempre interessante iniciar a análise com uma abordagem puramente de séries temporais (um modelo ARIMA, por exemplo). Outra estratégia e a de realizar comparações entre (1) uma previsão para dentro da amostra que inclua variáveis explicativas previstas e (2) uma previsão para dentro da amostra que inclua as mesmas variáveis explicativas com dados observados. 9.0.2 Modelos Dinâmicos de Regressão Se existem previsões precisas para as variáveis explicativas e a relação entre estas variáveis e a previsão é estável no futuro, podemos utilizar uma abordagem mista que envolve extender os modelos ARIMA com o objetivo de permitir que outras variáveis externas sejam incluídas nos modelos. Assim, teriamos o melhor dos dois mundos. Estes modelos de regressão simples tomam a forma \\[y_t = \\beta_0 + \\beta_1 x_{1,t} + ... + \\beta_k x_{k,t} + \\epsilon_t\\] onde \\(y_t\\) é uma função linear das \\(k\\) variáveis externas (\\(x_{1,t},...,x_{k,t}\\)), e \\(\\epsilon_t\\) é assumido como um termos de erro não correlacionado (ruído branco). Testes como de Breusch-Godfrey foram utilizados para assegurar que os resíduos resultantes da regressão eram significativamente correlacionados. Neste capítulo, os erros da regressão podem conter autocorrelação. Para enfatizar esta mudança, vamos substituir o uso do \\(\\epsilon_t\\) por \\(\\eta_t\\). A série de erros \\(\\eta_t\\) é assumido como um processo ARIMA. Por exemplo, se \\(\\eta_t\\) seguir um processo ARIMA(1,1,1), podemos escrever o modelo como \\[y_t = \\beta_0 + \\beta_1 x_{1,t} + ... + \\beta_k x_{k,t} + \\eta_t\\] \\[(1-\\Phi_1 B)(1-B)\\eta_t = (1+ \\theta_1 B) \\epsilon_t\\] onde \\(\\epsilon_t\\) é a série de ruído branco. Note que o modelo tem dois termos de erro - o erro do modelo de regressão, que denotamos como \\(\\eta_\\). e o termo de erro do modelo ARIMA, que denotamos como \\(\\epsilon_t\\). Apenas os erros do modelo ARIMA são assumidos como ruído branco. 9.1 Estimação Quando estimamos parâmetros do modelo, minimizamos a soma de \\(\\epsilon_t\\) ao quadrado. Se minimizarmos a soma de \\(\\eta_t\\) ao quadrado (que é o que ocorre quando estimamos um modelo de regressão que ignora a autocorrelação dos erros), então uma série de problemas surgem. Os coeficientes estimados \\(\\hat{\\beta}_0,...,\\hat{\\beta}_k\\) não são mais os melhores estimadores, já que algumas informações importantes estão sendo ignoradas no cálculo dos coeficientes. Qualquer teste estatístico associado com o modelo será incorreto. Os valores de AIC dos modelos ajustados não são um bom guia de quão bom é o modelo para previsão. Na maioria dos casos, o p-valor associado com os coeficientes será muito pequeno, e algumas covariadas parecerão importantes quando na verdade não são. Isto produzirá uma regressão espúria. Minimizar a soma dos \\(\\epsilon_t\\) ao quadrado evita estes problemas. Alternativamente, estimação por máxima verossimilhança pode ser utilizada, produzindo estimativas de coeficientes similares. Uma importante consideração quando estimando um modelo de regressão com erros ARMA é de que todas as variáveis do modelo devem ser estacionárias. Portanto, devemos primeiro checar se \\(y_t\\) é todas as covariadas são estacionárias. Se estimarmos o modelo quando qualquer uma delas é não-estacionária, os coeficientes produzidos não serão consistentes. Uma exceção é quando variáveis não-estacionárias são cointegradas. Se existe uma combinação linear de \\(y_t\\) não-estacionário com um \\(x_t\\) estacionário, então o coeficiente é consistente. Para tornar as variáveis estacionárias, podemos realizar a transformação de diferenciação, o que produz o chamado modelo em diferença, em contraste com o modelo em nível, em os dados originais são utilizados. Se todas as variáveis são estacionárias, então podemos utilizar erros ARMA para os resíduos. É fácil notar que uma regressão com erros ARIMA é equivalente a uma regressão em diferença com erros ARMA. 9.2 Regressão com Erros ARIMA A função Arima() é capaz de ajustar um modelo de regressão com erros ARIMA se o argumento xreg for utilizado. Como diferenciação está especificada, a diferenciação é aplicada para todas as variáveis antes de estimar o modelo. O comando R utilizado é library(forecast) Arima(y, xreg = x, order = c(1,1,0)) que irá ajustar um modelo do tipo \\(y&#39;_t = \\beta_1 x&#39;_t + \\eta&#39;_t\\). A função auto.arima() também é capaz de utilizar covariadas com uso do termo xreg. O usuário deve especificar os preditores e auto.arima() seleciona o melhor modelo ARIMA para os erros. 9.2.1 Exemplo: Consumo e Renda nos EUA A figura 9.1 mostra a mudança trimestral nos gastos com consumo pessoal e a renda disponível entre 1970 e 2016. Estamos interessados em prever o consumo com base na renda. Uma mudança na renda não necessariamente reflete uma mudança instântanea no consumo (exempl, depois de uma demissão, pode levar alguns meses para os gastos se ajustarem). Contudo, vamos ignorar esta complexidade e tentar medir o efeito instantâneo de uma mudança média na renda sore uma mudança média nos gastos. library(fpp) autoplot(usconsumption, facets = TRUE) + xlab(&quot;Ano&quot;) + ylab(&quot;&quot;) + ggtitle(&quot;Mudança Trimestral no Consumo e Renda, EUA&quot;) Figure 9.1: Mudança Percental no Consumo e Renda Trimestral para os EUA, 1970 a 2010 fit &lt;- auto.arima(usconsumption[,&quot;consumption&quot;], xreg=usconsumption[,&quot;income&quot;]) fit ## Series: usconsumption[, &quot;consumption&quot;] ## Regression with ARIMA(1,0,2) errors ## ## Coefficients: ## ar1 ma1 ma2 intercept xreg ## 0.6516 -0.5440 0.2187 0.5750 0.2420 ## s.e. 0.1468 0.1576 0.0790 0.0951 0.0513 ## ## sigma^2 estimated as 0.3502: log likelihood=-144.27 ## AIC=300.54 AICc=301.08 BIC=319.14 Os dados são claramente estacionários (já que estamos considerando mudanças percentuais em vez de gastos e renda bruta), de modo que não há necessidade de diferenciação. O modelo ajustado é \\[y_t = 0.5750 + 0.2420x_t + \\eta_t\\] \\[\\eta_t = 0.6516 \\eta_{t-1} + \\epsilon_t - 0.5440 + 0.218 \\epsilon_t\\] \\[\\epsilon ~ IID(0, 0.3502\\] Podemos recuperar as estimativas de \\(\\eta_t\\) e \\(\\epsilon_t\\) usando a função residuals(). cbind(&quot;Erros de Regressão&quot; = residuals(fit, type=&quot;regression&quot;), &quot;Erros ARIMA&quot; = residuals(fit, type=&quot;innovation&quot;)) %&gt;% autoplot(facets=T) Figure 9.2: Resíduos de Regressão e Resíduos ARIMA para o modelo ajustado O teste de Ljung-Box, ACF e histograma parecem indicar que os resíduos não são significativamente diferentes de um ruído branco. checkresiduals(fit) ## ## Ljung-Box test ## ## data: Residuals from Regression with ARIMA(1,0,2) errors ## Q* = 4.455, df = 3, p-value = 0.2163 ## ## Model df: 5. Total lags used: 8 References "],["modelos-avançados.html", "Capítulo 10 Modelos Avançados 10.1 Múltiplas Sazonalidades 10.2 Modelo Prophet 10.3 Modelo de Redes Neurais 10.4 Bootstrapping e Bagging", " Capítulo 10 Modelos Avançados 10.1 Múltiplas Sazonalidades 10.2 Modelo Prophet 10.3 Modelo de Redes Neurais 10.4 Bootstrapping e Bagging "],["modelos-hierárquicos.html", "Capítulo 11 Modelos Hierárquicos 11.1 Séries Temporais Agrupadas 11.2 Hierarquia Temporal", " Capítulo 11 Modelos Hierárquicos 11.1 Séries Temporais Agrupadas 11.2 Hierarquia Temporal https://cran.r-project.org/web/packages/thief/thief.pdf "],["references.html", "References", " References "]]
